---
title: "Bare Bones Model Examples"
author: Dr. Chelsea Parlett-Pelleriti
format:
    html:
        toc: true
        number-sections: true
        colorlinks: true
        number-depth: 3
jupyter: python3
---
# Overview
The following sections provide annotated bare-bones examples of how to run each algorithm we learn in class. This only includes data loading, pre-procesing, model building, and basic model evaluation and **does not** include all the extras we learn in class (like plotting, hyper-parameter tuning..etc).

# Supervised Machine Learning
In supervised ML the general workflow is:


- Create `X` and `y` (includes pre-processing, model validation...etc)
- Create empty model
- `.fit()`
- `.predict()`
- Evaluate

## Regression (Continuous Outcome)

<details>
<summary>Linear Regression (No Model Validation)</summary>

#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.linear_model import LinearRegression # Linear Regression Model
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`. 

```{python}
continuous = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X[continuous]) # z score all predictors that are continuous/interval
X[continuous] = z.transform(X[continuous]) # z score all predictors that are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
# create model
LR_Model = LinearRegression() # empty model

# fit model
LR_Model.fit(X, y) # fit model (aka learn the model parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics.

```{python}
# predict
price_pred = LR_Model.predict(X) # make predictions for data in X

# mse and mae
print("MSE: ", mean_squared_error(y,price_pred))
print("MAE: ", mean_absolute_error(y,price_pred))

# r2
print("R2 : ", r2_score(y,price_pred))
```
</details> 


<details>
<summary>Linear Regression</summary>

<section>

#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.linear_model import LinearRegression # Linear Regression Model
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

```{python}
continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
# create model
LR_Model = LinearRegression() # empty model

# fit model
LR_Model.fit(X_train, y_train) # fit model (aka learn the model parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
price_pred_train = LR_Model.predict(X_train) # make predictions for data in X

# mse and mae
print("Train MSE: ", mean_squared_error(y_train,price_pred_train))
print("Train MAE: ", mean_absolute_error(y_train,price_pred_train))

# r2
print("Train R2 : ", r2_score(y_train,price_pred_train))

# predict
price_pred_test = LR_Model.predict(X_test) # make predictions for data in X

# mse and mae
print("Test MSE : ", mean_squared_error(y_test,price_pred_test))
print("Test MAE : ", mean_absolute_error(y_test,price_pred_test))

# r2
print("Test R2  : ", r2_score(y_test,price_pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-squared (R2) values. Because we're doing K-Fold CV, we'll have K different MSEs, MAEs, and R2s for the train/test set. So, we store all K of them in lists (`mae_train`, `mae_test`...etc).

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

mse_train = []
mse_test  = []
mae_train = []
mae_test  = []
r2_train   = []
r2_test    = []

continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    # create model
    LR_Model = LinearRegression() # empty model

    # fit model
    LR_Model.fit(X_train, y_train) # fit model (aka learn the model parameters)
    
    # predict
    price_pred_train = LR_Model.predict(X_train) # make predictions for data in X_train
    price_pred_test = LR_Model.predict(X_test) # make predictions for data in X_test
    
    # mse
    mse_train.append(mean_squared_error(y_train, price_pred_train))
    mse_test.append(mean_squared_error(y_test, price_pred_test))
    
    # mae
    mae_train.append(mean_absolute_error(y_train, price_pred_train))
    mae_test.append(mean_absolute_error(y_test, price_pred_test))
    
    #r2
    r2_train.append(r2_score(y_train, price_pred_train))
    r2_test.append(r2_score(y_test, price_pred_test))

print("MSE train: ", np.mean(mse_train))
print("MSE test : ", np.mean(mse_test))   

print("MAE train: ", np.mean(mae_train))
print("MAE test : ", np.mean(mae_test))   

print("R2 train : ", np.mean(r2_train))
print("R2 test  : ", np.mean(r2_test))   
    
    
```

:::
</details>


<details>
<summary>K-Nearest Neighbors Regression</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.neighbors import KNeighborsRegressor # KNN Regression Model
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

```{python}
continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
# create model
KNN = KNeighborsRegressor() # empty model

# fit model
KNN.fit(X_train, y_train) # fit model (aka learn the model parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
price_pred_train = KNN.predict(X_train) # make predictions for data in X

# mse and mae
print("Train MSE: ", mean_squared_error(y_train,price_pred_train))
print("Train MAE: ", mean_absolute_error(y_train,price_pred_train))

# r2
print("Train R2 : ", r2_score(y_train,price_pred_train))

# predict
price_pred_test = KNN.predict(X_test) # make predictions for data in X

# mse and mae
print("Test MSE : ", mean_squared_error(y_test,price_pred_test))
print("Test MAE : ", mean_absolute_error(y_test,price_pred_test))

# r2
print("Test R2  : ", r2_score(y_test,price_pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-squared (R2) values. Because we're doing K-Fold CV, we'll have K different MSEs, MAEs, and R2s for the train/test set. So, we store all K of them in lists (`mae_train`, `mae_test`...etc).

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

mse_train = []
mse_test  = []
mae_train = []
mae_test  = []
r2_train   = []
r2_test    = []

continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    # create model
    KNN = KNeighborsRegressor() # empty model

    # fit model
    KNN.fit(X_train, y_train) # fit model (aka learn the model parameters)
    
    # predict
    price_pred_train = KNN.predict(X_train) # make predictions for data in X_train
    price_pred_test = KNN.predict(X_test) # make predictions for data in X_test
    
    # mse
    mse_train.append(mean_squared_error(y_train, price_pred_train))
    mse_test.append(mean_squared_error(y_test, price_pred_test))
    
    # mae
    mae_train.append(mean_absolute_error(y_train, price_pred_train))
    mae_test.append(mean_absolute_error(y_test, price_pred_test))
    
    #r2
    r2_train.append(r2_score(y_train, price_pred_train))
    r2_test.append(r2_score(y_test, price_pred_test))

print("MSE train: ", np.mean(mse_train))
print("MSE test : ", np.mean(mse_test))   

print("MAE train: ", np.mean(mae_train))
print("MAE test : ", np.mean(mae_test))   

print("R2 train : ", np.mean(r2_train))
print("R2 test  : ", np.mean(r2_test))   
    
    
```

:::
</details>

<details>
<summary>Regression Trees</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.tree import DecisionTreeRegressor # Regression Tree Model
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

```{python}
continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
# create model
tree = DecisionTreeRegressor() # empty model

# fit model
tree.fit(X_train, y_train) # fit model (aka learn the model parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
price_pred_train = tree.predict(X_train) # make predictions for data in X

# mse and mae
print("Train MSE: ", mean_squared_error(y_train,price_pred_train))
print("Train MAE: ", mean_absolute_error(y_train,price_pred_train))

# r2
print("Train R2 : ", r2_score(y_train,price_pred_train))

# predict
price_pred_test = tree.predict(X_test) # make predictions for data in X

# mse and mae
print("Test MSE : ", mean_squared_error(y_test,price_pred_test))
print("Test MAE : ", mean_absolute_error(y_test,price_pred_test))

# r2
print("Test R2  : ", r2_score(y_test,price_pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-squared (R2) values. Because we're doing K-Fold CV, we'll have K different MSEs, MAEs, and R2s for the train/test set. So, we store all K of them in lists (`mae_train`, `mae_test`...etc).

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

mse_train = []
mse_test  = []
mae_train = []
mae_test  = []
r2_train   = []
r2_test    = []

continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    # create model
    tree = DecisionTreeRegressor() # empty model

    # fit model
    tree.fit(X_train, y_train) # fit model (aka learn the model parameters)
    
    # predict
    price_pred_train = tree.predict(X_train) # make predictions for data in X_train
    price_pred_test = tree.predict(X_test) # make predictions for data in X_test
    
    # mse
    mse_train.append(mean_squared_error(y_train, price_pred_train))
    mse_test.append(mean_squared_error(y_test, price_pred_test))
    
    # mae
    mae_train.append(mean_absolute_error(y_train, price_pred_train))
    mae_test.append(mean_absolute_error(y_test, price_pred_test))
    
    #r2
    r2_train.append(r2_score(y_train, price_pred_train))
    r2_test.append(r2_score(y_test, price_pred_test))

print("MSE train: ", np.mean(mse_train))
print("MSE test : ", np.mean(mse_test))   

print("MAE train: ", np.mean(mae_train))
print("MAE test : ", np.mean(mae_test))   

print("R2 train : ", np.mean(r2_train))
print("R2 test  : ", np.mean(r2_test))   
    
    
```

:::
</details>


<details>
<summary>Random Forest Regression</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.ensemble import RandomForestRegressor # Regression RF Model
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

```{python}
continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
# create model
tree = RandomForestRegressor() # empty model

# fit model
tree.fit(X_train, y_train) # fit model (aka learn the model parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
price_pred_train = tree.predict(X_train) # make predictions for data in X

# mse and mae
print("Train MSE: ", mean_squared_error(y_train,price_pred_train))
print("Train MAE: ", mean_absolute_error(y_train,price_pred_train))

# r2
print("Train R2 : ", r2_score(y_train,price_pred_train))

# predict
price_pred_test = tree.predict(X_test) # make predictions for data in X

# mse and mae
print("Test MSE : ", mean_squared_error(y_test,price_pred_test))
print("Test MAE : ", mean_absolute_error(y_test,price_pred_test))

# r2
print("Test R2  : ", r2_score(y_test,price_pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-squared (R2) values. Because we're doing K-Fold CV, we'll have K different MSEs, MAEs, and R2s for the train/test set. So, we store all K of them in lists (`mae_train`, `mae_test`...etc).

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

mse_train = []
mse_test  = []
mae_train = []
mae_test  = []
r2_train   = []
r2_test    = []

continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    # create model
    tree = RandomForestRegressor() # empty model

    # fit model
    tree.fit(X_train, y_train) # fit model (aka learn the model parameters)
    
    # predict
    price_pred_train = tree.predict(X_train) # make predictions for data in X_train
    price_pred_test = tree.predict(X_test) # make predictions for data in X_test
    
    # mse
    mse_train.append(mean_squared_error(y_train, price_pred_train))
    mse_test.append(mean_squared_error(y_test, price_pred_test))
    
    # mae
    mae_train.append(mean_absolute_error(y_train, price_pred_train))
    mae_test.append(mean_absolute_error(y_test, price_pred_test))
    
    #r2
    r2_train.append(r2_score(y_train, price_pred_train))
    r2_test.append(r2_score(y_test, price_pred_test))

print("MSE train: ", np.mean(mse_train))
print("MSE test : ", np.mean(mse_test))   

print("MAE train: ", np.mean(mae_train))
print("MAE test : ", np.mean(mae_test))   

print("R2 train : ", np.mean(r2_train))
print("R2 test  : ", np.mean(r2_test))   
    
    
```

:::
</details>

<details>
<summary>LASSO</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.linear_model import Lasso # Lasso Model
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

```{python}
continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
# create model
lasso = Lasso() # empty model

# fit model
lasso.fit(X_train, y_train) # fit model (aka learn the model parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
price_pred_train = lasso.predict(X_train) # make predictions for data in X

# mse and mae
print("Train MSE: ", mean_squared_error(y_train,price_pred_train))
print("Train MAE: ", mean_absolute_error(y_train,price_pred_train))

# r2
print("Train R2 : ", r2_score(y_train,price_pred_train))

# predict
price_pred_test = lasso.predict(X_test) # make predictions for data in X

# mse and mae
print("Test MSE : ", mean_squared_error(y_test,price_pred_test))
print("Test MAE : ", mean_absolute_error(y_test,price_pred_test))

# r2
print("Test R2  : ", r2_score(y_test,price_pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-squared (R2) values. Because we're doing K-Fold CV, we'll have K different MSEs, MAEs, and R2s for the train/test set. So, we store all K of them in lists (`mae_train`, `mae_test`...etc).

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

mse_train = []
mse_test  = []
mae_train = []
mae_test  = []
r2_train   = []
r2_test    = []

continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    # create model
    lasso = Lasso() # empty model

    # fit model
    lasso.fit(X_train, y_train) # fit model (aka learn the model parameters)
    
    # predict
    price_pred_train = lasso.predict(X_train) # make predictions for data in X_train
    price_pred_test = lasso.predict(X_test) # make predictions for data in X_test
    
    # mse
    mse_train.append(mean_squared_error(y_train, price_pred_train))
    mse_test.append(mean_squared_error(y_test, price_pred_test))
    
    # mae
    mae_train.append(mean_absolute_error(y_train, price_pred_train))
    mae_test.append(mean_absolute_error(y_test, price_pred_test))
    
    #r2
    r2_train.append(r2_score(y_train, price_pred_train))
    r2_test.append(r2_score(y_test, price_pred_test))

print("MSE train: ", np.mean(mse_train))
print("MSE test : ", np.mean(mse_test))   

print("MAE train: ", np.mean(mae_train))
print("MAE test : ", np.mean(mae_test))   

print("R2 train : ", np.mean(r2_train))
print("R2 test  : ", np.mean(r2_test))   
    
    
```

:::
</details>


<details>
<summary>Ridge</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.linear_model import Ridge # Ridge Model
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

```{python}
continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
# create model
ridge = Ridge() # empty model

# fit model
ridge.fit(X_train, y_train) # fit model (aka learn the model parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
price_pred_train = ridge.predict(X_train) # make predictions for data in X

# mse and mae
print("Train MSE: ", mean_squared_error(y_train,price_pred_train))
print("Train MAE: ", mean_absolute_error(y_train,price_pred_train))

# r2
print("Train R2 : ", r2_score(y_train,price_pred_train))

# predict
price_pred_test = ridge.predict(X_test) # make predictions for data in X

# mse and mae
print("Test MSE : ", mean_squared_error(y_test,price_pred_test))
print("Test MAE : ", mean_absolute_error(y_test,price_pred_test))

# r2
print("Test R2  : ", r2_score(y_test,price_pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-squared (R2) values. Because we're doing K-Fold CV, we'll have K different MSEs, MAEs, and R2s for the train/test set. So, we store all K of them in lists (`mae_train`, `mae_test`...etc).

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

mse_train = []
mse_test  = []
mae_train = []
mae_test  = []
r2_train   = []
r2_test    = []

continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    # create model
    ridge = Ridge() # empty model

    # fit model
    ridge.fit(X_train, y_train) # fit model (aka learn the model parameters)
    
    # predict
    price_pred_train = ridge.predict(X_train) # make predictions for data in X_train
    price_pred_test = ridge.predict(X_test) # make predictions for data in X_test
    
    # mse
    mse_train.append(mean_squared_error(y_train, price_pred_train))
    mse_test.append(mean_squared_error(y_test, price_pred_test))
    
    # mae
    mae_train.append(mean_absolute_error(y_train, price_pred_train))
    mae_test.append(mean_absolute_error(y_test, price_pred_test))
    
    #r2
    r2_train.append(r2_score(y_train, price_pred_train))
    r2_test.append(r2_score(y_test, price_pred_test))

print("MSE train: ", np.mean(mse_train))
print("MSE test : ", np.mean(mse_test))   

print("MAE train: ", np.mean(mae_train))
print("MAE test : ", np.mean(mae_test))   

print("R2 train : ", np.mean(r2_train))
print("R2 test  : ", np.mean(r2_test))   
    
    
```

:::
</details>

<details>
<summary>Neural Network</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
import tensorflow.keras as kb
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
ama = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/amazon-books.txt",
                 sep = "\t") #file is tsv not csv, so sep = "\t"

# ama.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(ama.isnull().sum()) # check if too many are missing
print(ama.shape) # how many rows, cols to we have?
```

There aren't many missing values here, compared to the number of rows, so I feel comfortable dropping any row that's missing a value. So let's do that.

```{python}
# drop missing
ama = ama.dropna() # drop missing rows
ama = ama.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also turn `Hard/ Paper` into a binary (0/1) variable so that our model can process it.

```{python}
# set up X and y
predictors = ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width", "Hard/ Paper"] # specify predictor vars

X = ama[predictors] # predictors/input
y = ama["Amazon Price"] # outcome/output

# binary
b = LabelBinarizer()
X["Hard/ Paper"] = b.fit_transform(X["Hard/ Paper"])
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

```{python}
continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
#structure of the model
model = kb.Sequential([
    kb.layers.Dense(1, input_shape =[7]), #input
])

#how to train the model
model.compile(loss = "mean_squared_error",
              optimizer = kb.optimizers.SGD())

#fit the model (same as SKlearn)
model.fit(X_train,y_train, epochs = 5)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-sqaured (R2) values.

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
price_pred_train = model.predict(X_train) # make predictions for data in X

# mse and mae
print("Train MSE: ", mean_squared_error(y_train,price_pred_train))
print("Train MAE: ", mean_absolute_error(y_train,price_pred_train))

# r2
print("Train R2 : ", r2_score(y_train,price_pred_train))

# predict
price_pred_test = model.predict(X_test) # make predictions for data in X

# mse and mae
print("Test MSE : ", mean_squared_error(y_test,price_pred_test))
print("Test MAE : ", mean_absolute_error(y_test,price_pred_test))

# r2
print("Test R2  : ", r2_score(y_test,price_pred_test))
```



### K-Fold

**NOTE**: We're building a pretty simple feed forward NN here, once the models get a little bigger, K-Fold can sometimes be too computationally expensive to run.

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a continuous outcome, we often check the Mean Squared Error (MSE), Mean Absolute Error (MAE) and R-squared (R2) values. Because we're doing K-Fold CV, we'll have K different MSEs, MAEs, and R2s for the train/test set. So, we store all K of them in lists (`mae_train`, `mae_test`...etc).

All three of these metrics compare the *actual* outcomes (here, Amazon Price) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

mse_train = []
mse_test  = []
mae_train = []
mae_test  = []
r2_train   = []
r2_test    = []

continuous =  ["List Price", "NumPages",
              "Weight (oz)", "Thick",
              "Height", "Width"] # specify continuous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    #structure of the model
    model = kb.Sequential([
        kb.layers.Dense(1, input_shape =[7]), #input
    ])
    
    #how to train the model
    model.compile(loss = "mean_squared_error",
                  optimizer = kb.optimizers.SGD())
    
    #fit the model (same as SKlearn)
    model.fit(X_train,y_train, epochs = 5)
    
    # predict
    price_pred_train = model.predict(X_train) # make predictions for data in X_train
    price_pred_test = model.predict(X_test) # make predictions for data in X_test
    
    # mse
    mse_train.append(mean_squared_error(y_train, price_pred_train))
    mse_test.append(mean_squared_error(y_test, price_pred_test))
    
    # mae
    mae_train.append(mean_absolute_error(y_train, price_pred_train))
    mae_test.append(mean_absolute_error(y_test, price_pred_test))
    
    #r2
    r2_train.append(r2_score(y_train, price_pred_train))
    r2_test.append(r2_score(y_test, price_pred_test))

print("MSE train: ", np.mean(mse_train))
print("MSE test : ", np.mean(mse_test))   

print("MAE train: ", np.mean(mae_train))
print("MAE test : ", np.mean(mae_test))   

print("R2 train : ", np.mean(r2_train))
print("R2 test  : ", np.mean(r2_test))   
    
    
```

:::
</details>


## Classification (Categorical Outcome)

<details>
<summary>Logistic Regression</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

```

```{python}
heart = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/heart.csv")

# heart.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(heart.isnull().sum()) # check if too many are missing
print(heart.shape) # how many rows, cols to we have?
```

There aren't any missing values here, but we'll go through the motions anyway (it won't negatively impact our analysis).

```{python}
# drop missing
heart = heart.dropna() # drop missing rows
heart = heart.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also create dummy variables for our categorical/ordinal variables if they have more than 2 (0/1) categories.

```{python}
# set up X and y
predictors = ["age","sex","cp",
"trestbps","chol","fbs","restecg",
"thalach","exang","oldpeak","slope"
,"ca","thal"] # specify predictor vars

X = heart[predictors] # predictors/input
y = heart["target"] # outcome/output

# dummy

X = pd.get_dummies(X, columns= ["cp", "slope", "thal"], drop_first = True)
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. Only `age`, `testbps`, `chol`, `thalach` and `oldpeak` are continuous, so we'll z score those.

```{python}
continuous = ["age", "trestbps","chol", "thalach","oldpeak"] # specify continous vars
# z score
z = StandardScaler() # create empty z-score object

z.fit(X_train[continuous]) # all predictors are continuous/interval
X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
```{python}
lr = LogisticRegression() # create empty model

lr.fit(X_train, y_train) # fit model (aka find the best parameters)
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a categorical outcome, we often check the Accuracy, ROC AUC, and confusion matrix (among others...)

All of these metrics compare the *actual* outcomes (here, `target`: whether or not someone had a cardiac event) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
pred_train = lr.predict(X_train) # make predictions for data in X_train
pred_train_proba = lr.predict_proba(X_train)[:,1] # make probabilities for data in X_train

# acc and auc
print("Train Acc: ", accuracy_score(y_train,pred_train))
print("Train AUC: ", roc_auc_score(y_train,pred_train_proba))

# confusion matrix
print("Train CM : ", confusion_matrix(y_train,pred_train))

# predict
pred_test = lr.predict(X_test) # make predictions for data in X_test
pred_test_proba = lr.predict_proba(X_test)[:,1] # make probabilities for data in X_test


# acc and auc
print("Test Acc : ", accuracy_score(y_test,pred_test))
print("Test AUC : ", roc_auc_score(y_test,pred_test_proba))

# confusion matrix
print("Test CM  : ", confusion_matrix(y_test,pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll z-score our inputs. Only `age`, `testbps`, `chol`, `thalach` and `oldpeak` are continuous, so we'll z score those.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. 
Now that we have a fitted model, we need to check how well it performs. For a categorical outcome, we often check the Accuracy, ROC AUC, and confusion matrix (among others...). Because we're doing K-Fold CV, we'll have K different Accuracies, ROC AUCs, and Confusion Matrices for the train/test set. So, we store all K of them in lists (`acc_train`, `acc_test`...etc), and print out the confusion matrices for each model.

All three of these metrics compare the *actual* outcomes (here, `target`: whether or not someone had a cardiac event) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

acc_train = []
acc_test  = []
auc_train = []
auc_test  = []

continuous = ["age", "trestbps","chol", "thalach","oldpeak"] # specify continous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    # z score
    z = StandardScaler() # create empty z-score object

    z.fit(X_train[continuous]) # all predictors are continuous/interval
    X_train[continuous] = z.transform(X_train[continuous]) # all predictors are continuous/interval
    X_test[continuous] = z.transform(X_test[continuous]) # all predictors are continuous/interval
    
    # create empty model
    lr = LogisticRegression()

    # fit model
    lr.fit(X_train,y_train)
    
    # predict
    pred_train = lr.predict(X_train) # make predictions for data in X_train
    pred_train_proba = lr.predict_proba(X_train)[:,1] # make probabilities for data in X_train

    pred_test = lr.predict(X_test) # make predictions for data in X_test
    pred_test_proba = lr.predict_proba(X_test)[:,1] # make probabilities for data in X_test
    
    # acc
    acc_train.append(accuracy_score(y_train, pred_train))
    acc_test.append(accuracy_score(y_test, pred_test))
    
    # auc
    auc_train.append(roc_auc_score(y_train, pred_train_proba))
    auc_test.append(roc_auc_score(y_test, pred_test_proba))
    
    # cm
    print(confusion_matrix(y_train, pred_train))
    print(confusion_matrix(y_test, pred_test))

print("Acc train: ", np.mean(acc_train))
print("Acc test : ", np.mean(acc_test))   

print("AUC train: ", np.mean(auc_train))
print("AUC test : ", np.mean(auc_test))   

    
    
```

:::
</details>

<details>
<summary>KNN</summary>

<section>
#### Loading and Pre-Processing
First, we need to import our packages and load in our data.
```{python}
# doesn't show warnings so that we can have blissful ignorance
import warnings
warnings.filterwarnings('ignore')

# base packages
import pandas as pd
import numpy as np
from plotnine import *

# modelling imports
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, LabelBinarizer #Z-score variables, binary
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix #model evaluation

from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer

```

```{python}
heart = pd.read_csv("https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/heart.csv")

# heart.head()
```

Now we want to do any pre-processing we need. Here we'll check the number of missing values to see if we need to drop any.


```{python}
# check null
print(heart.isnull().sum()) # check if too many are missing
print(heart.shape) # how many rows, cols to we have?
```

There aren't any missing values here, but we'll go through the motions anyway (it won't negatively impact our analysis).

```{python}
# drop missing
heart = heart.dropna() # drop missing rows
heart = heart.reset_index() # reset indices
```

Here we drop missing values, and reset the indices. `dropna()` will by default drop any row that's missing a value. Say we have 5 rows, and our row indices are 0,1,2,3 and 4. If row 2 has a missing value, `dropna()` will drop it. But now our indices are weird, because it goes 0,1,3,4 instead of 0,1,2,3. `reset_index()` will take our data and make sure there are no skipped indices.

Now we can prepare to build our model by separating the data into predictors (inputs to the model) and an outcome (the output of the model; what it's predicting). We'll also create dummy variables for our categorical/ordinal variables if they have more than 2 (0/1) categories.

```{python}
# set up X and y
predictors = ["age","sex","cp",
"trestbps","chol","fbs","restecg",
"thalach","exang","oldpeak","slope"
,"ca","thal"] # specify predictor vars

X = heart[predictors] # predictors/input
y = heart["target"] # outcome/output

# dummy

X = pd.get_dummies(X, columns= ["cp", "slope", "thal"], drop_first = True)
```

Next, we need to separate our data into a *training* and *testing* set. The train set will be used to fit the model, and the test set will be used to help us understand how the model performs on data it has never seen before. We are VERY careful to make sure that no information from the test set leaks into the model.

:::panel-tabset

### Train Test Split

```{python}
X_test, X_train, y_test, y_train = train_test_split(X,y, test_size = 0.2) #80/20 TTS
```

Then, we'll z-score our inputs. All our predictors except `Hard/ Paper` are continuous, so we'll z-score all of them except `Hard/ Paper`. Because we're going to do hyperparameter tuning to choose `k` (the number of neighbors to look at), we'll create a z-score object using `make_column_transformer`. This function takes in two *very* important arguments. 

- first, it takes in a *tuple* that contains both the transformation we want to apply (here: `StandardScaler()`), and the columns we want to transform with that transformation (here: `continuous`)
- `remainder = "passthrough"` which tells python not to drop/remove all the non-z-scored columns 

```{python}
continuous = ["age", "trestbps","chol", "thalach","oldpeak"] # specify continous vars

# z score
z = make_column_transformer((StandardScaler(), continuous),
remainder = "passthrough")

```

#### Build and Fit the Model
Now, we're ready to build the model. First, we create an empty model and put it and our `z` object into a pipeline, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. *However*, because we want to do hyperparameter tuning via `GridSearchCV`, we'll need to first create a grid search object, and tell it which hyperparameters to try (these hyperparameter options are stored in the dictionary `ks`).
```{python}

knn = KNeighborsClassifier() # create empty model

# pipeline

pipe = make_pipeline(z,knn)

# hyperparams
ks = {"kneighborsclassifier__n_neighbors": range(1,31)}

# grid
grid = GridSearchCV(pipe, ks, scoring = "accuracy", cv = 5, refit = True)


grid.fit(X_train, y_train) # fit model (aka find the best parameters)

# what k did it choose??
print("k chosen:", grid.best_estimator_.get_params()["kneighborsclassifier__n_neighbors"])
```

#### Evaluate the Model

Now that we have a fitted model, we need to check how well it performs. For a categorical outcome, we often check the Accuracy, ROC AUC, and confusion matrix (among others...)

All of these metrics compare the *actual* outcomes (here, `target`: whether or not someone had a cardiac event) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.

```{python}
# predict
pred_train = grid.predict(X_train) # make predictions for data in X_train
pred_train_proba = grid.predict_proba(X_train)[:,1] # make probabilities for data in X_train

# acc and auc
print("Train Acc: ", accuracy_score(y_train,pred_train))
print("Train AUC: ", roc_auc_score(y_train,pred_train_proba))

# confusion matrix
print("Train CM : ", confusion_matrix(y_train,pred_train))

# predict
pred_test = grid.predict(X_test) # make predictions for data in X_test
pred_test_proba = grid.predict_proba(X_test)[:,1] # make probabilities for data in X_test


# acc and auc
print("Test Acc : ", accuracy_score(y_test,pred_test))
print("Test AUC : ", roc_auc_score(y_test,pred_test_proba))

# confusion matrix
print("Test CM  : ", confusion_matrix(y_test,pred_test))
```



### K-Fold

To use KFold, we need to create a `for` loop which repeats our code `K` times (here, `K` is 5, for 5-Fold CV). The variables `train` and `test` contain the indices of the rows that should be in the train and test sets, respectively. Each time our for loop iterates, we will grab the rows in `X` that should be in the training set and put them in `X_train`, we will grab the rows in `X` that should be in the testing set and put them in `X_test`, we will grab the rows in `y` that should be in the training set and put them in `y_train`, and we will grab the rows in `y` that should be in the testing set and put them in `y_test`.

Then, we'll create a z-score object. Only `age`, `testbps`, `chol`, `thalach` and `oldpeak` are continuous, so we'll z score those, the rest will be left alone because we set `remainder = "passthrough"`.

#### Model Building and Evaluation 
Now, we're ready to build the model. First, we create an empty model and put it and our `z` object into a pipeline, and then we fit it. Fitting allows the empty model to use the data we give it (here `X` and `y`) to figure out what the parameters of the model should be. *However*, because we want to do hyperparameter tuning via `GridSearchCV`, we'll need to first create a grid search object, and tell it which hyperparameters to try (these hyperparameter options are stored in the dictionary `ks`).

Now that we have a fitted model, we need to check how well it performs. For a categorical outcome, we often check the Accuracy, ROC AUC, and confusion matrix (among others...). Because we're doing K-Fold CV, we'll have K different Accuracies, ROC AUCs, and Confusion Matrices for the train/test set. So, we store all K of them in lists (`acc_train`, `acc_test`...etc), and print out the confusion matrices for each model.

All three of these metrics compare the *actual* outcomes (here, `target`: whether or not someone had a cardiac event) with what the model *predicts* the outcome will be. So first, we generate predictions for our dataset, then we use these predictions and the actual values (in `y`) to calculate these three metrics. We'll do this for both the train and the test set.


```{python}
kf = KFold(5, shuffle = True) # 5-Fold CV

acc_train = []
acc_test  = []
auc_train = []
auc_test  = []

continuous = ["age", "trestbps","chol", "thalach","oldpeak"] # specify continous vars

for train,test in kf.split(X):
    
    # split into train/test
    X_train = X.iloc[train]
    X_test = X.iloc[test]
    y_train = y[train]
    y_test = y[test]
    
    continuous = ["age", "trestbps","chol", "thalach","oldpeak"] # specify continous vars

    # z score
    z = make_column_transformer((StandardScaler(), continuous),
    remainder = "passthrough")

    # model
    knn = KNeighborsClassifier() # create empty model

    # pipeline
    pipe = make_pipeline(z,knn)

    # hyperparams
    ks = {"kneighborsclassifier__n_neighbors": range(1,31)}

    # grid
    grid = GridSearchCV(pipe, ks, scoring = "accuracy", cv = 5, refit = True)


    grid.fit(X_train, y_train) # fit model (aka find the best parameters)

    # what k did it choose??
    print("k chosen: ", grid.best_estimator_.get_params()["kneighborsclassifier__n_neighbors"])
    
    # predict
    pred_train = grid.predict(X_train) # make predictions for data in X_train
    pred_train_proba = grid.predict_proba(X_train)[:,1] # make probabilities for data in X_train

    pred_test = grid.predict(X_test) # make predictions for data in X_test
    pred_test_proba = grid.predict_proba(X_test)[:,1] # make probabilities for data in X_test
    
    # acc
    acc_train.append(accuracy_score(y_train, pred_train))
    acc_test.append(accuracy_score(y_test, pred_test))
    
    # auc
    auc_train.append(roc_auc_score(y_train, pred_train_proba))
    auc_test.append(roc_auc_score(y_test, pred_test_proba))
    
    # cm
    print(confusion_matrix(y_train, pred_train))
    print(confusion_matrix(y_test, pred_test))

print("Acc train: ", np.mean(acc_train))
print("Acc test : ", np.mean(acc_test))   

print("AUC train: ", np.mean(auc_train))
print("AUC test : ", np.mean(auc_test))   

    
    
```

:::
</details>
### Naive Bayes
### Decision Trees
### Random Forest
### Neural Network

# Unsupervised Machine Learning

## Clustering
### KMeans
### Gaussian Mixture Model
### DBSCAN
### Hierarchical Agglomerative Clustering

## Dimensionality Reduction
### PCA


